{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holographic Behavioral Twin (HBT) Visualization Demo\n",
    "\n",
    "This notebook demonstrates the key concepts behind HBT validation through interactive visualizations.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "1. **Hyperdimensional Computing (HDC)**: How behavioral responses are encoded into high-dimensional vectors\n",
    "2. **Variance Tensor Visualization**: Understanding behavioral variance patterns across different model layers\n",
    "3. **Causal Graph Discovery**: How HBT infers structural relationships in model behavior\n",
    "4. **Model Comparison**: Visual comparison of behavioral signatures between different models\n",
    "5. **REV Executor**: Memory-bounded execution patterns for scalable analysis\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install jupyter matplotlib seaborn plotly networkx pandas numpy scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import HBT components\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().parent))\n",
    "\n",
    "from core.hdc_encoder import HyperdimensionalEncoder\n",
    "from core.hbt_constructor import HolographicBehavioralTwin\n",
    "from challenges.probe_generator import ProbeGenerator\n",
    "from core.variance_analyzer import VarianceAnalyzer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üé® HBT Visualization Demo Ready!\")\n",
    "print(\"üìä All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üß† Hyperdimensional Computing Visualization\n",
    "\n",
    "Let's start by understanding how behavioral responses are encoded into hyperdimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HDC encoder\n",
    "encoder = HyperdimensionalEncoder(\n",
    "    dimension=1024,  # Smaller for visualization\n",
    "    sparsity=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"‚ú® Created HDC encoder with {encoder.dimension} dimensions\")\n",
    "print(f\"üéØ Sparsity level: {encoder.sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample probes and responses\n",
    "sample_probes = [\n",
    "    {\"text\": \"Explain quantum computing\", \"domain\": \"science\", \"complexity\": 4},\n",
    "    {\"text\": \"Write a simple Python function\", \"domain\": \"code\", \"complexity\": 2},\n",
    "    {\"text\": \"What is the meaning of life?\", \"domain\": \"philosophy\", \"complexity\": 5},\n",
    "    {\"text\": \"Calculate 2 + 2\", \"domain\": \"math\", \"complexity\": 1},\n",
    "    {\"text\": \"Describe the water cycle\", \"domain\": \"science\", \"complexity\": 3}\n",
    "]\n",
    "\n",
    "sample_responses = [\n",
    "    {\"text\": \"Quantum computing uses quantum mechanical phenomena...\", \"tokens\": [\"Quantum\", \"computing\", \"uses\"], \"logprobs\": [-0.1, -0.3, -0.2]},\n",
    "    {\"text\": \"def simple_function():\\n    return 'Hello World'\", \"tokens\": [\"def\", \"simple\", \"function\"], \"logprobs\": [-0.05, -0.2, -0.15]},\n",
    "    {\"text\": \"The meaning of life is a profound philosophical question...\", \"tokens\": [\"The\", \"meaning\", \"of\"], \"logprobs\": [-0.1, -0.25, -0.1]},\n",
    "    {\"text\": \"2 + 2 = 4\", \"tokens\": [\"2\", \"+\", \"2\"], \"logprobs\": [-0.01, -0.01, -0.01]},\n",
    "    {\"text\": \"The water cycle involves evaporation, condensation...\", \"tokens\": [\"The\", \"water\", \"cycle\"], \"logprobs\": [-0.08, -0.12, -0.18]}\n",
    "]\n",
    "\n",
    "# Encode probes and responses\n",
    "probe_vectors = []\n",
    "response_vectors = []\n",
    "\n",
    "for probe, response in zip(sample_probes, sample_responses):\n",
    "    probe_hv = encoder.probe_to_hypervector(probe)\n",
    "    response_hv = encoder.response_to_hypervector(response)\n",
    "    \n",
    "    probe_vectors.append(probe_hv)\n",
    "    response_vectors.append(response_hv)\n",
    "\n",
    "probe_vectors = np.array(probe_vectors)\n",
    "response_vectors = np.array(response_vectors)\n",
    "\n",
    "print(f\"üìä Encoded {len(sample_probes)} probe-response pairs\")\n",
    "print(f\"üìê Probe vectors shape: {probe_vectors.shape}\")\n",
    "print(f\"üìê Response vectors shape: {response_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hypervector properties\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Hyperdimensional Vector Properties', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Sparsity patterns\n",
    "sample_hv = probe_vectors[0]\n",
    "axes[0,0].hist(sample_hv, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribution of Values in Hypervector')\n",
    "axes[0,0].set_xlabel('Value')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].axvline(0, color='red', linestyle='--', label='Zero (sparse)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Similarity heatmap\n",
    "similarities = np.zeros((len(probe_vectors), len(probe_vectors)))\n",
    "for i in range(len(probe_vectors)):\n",
    "    for j in range(len(probe_vectors)):\n",
    "        similarities[i,j] = encoder.compute_similarity(probe_vectors[i], probe_vectors[j])\n",
    "\n",
    "domains = [probe['domain'] for probe in sample_probes]\n",
    "im = axes[0,1].imshow(similarities, cmap='viridis')\n",
    "axes[0,1].set_title('Probe Similarity Matrix')\n",
    "axes[0,1].set_xticks(range(len(domains)))\n",
    "axes[0,1].set_yticks(range(len(domains)))\n",
    "axes[0,1].set_xticklabels(domains, rotation=45)\n",
    "axes[0,1].set_yticklabels(domains)\n",
    "plt.colorbar(im, ax=axes[0,1], label='Similarity')\n",
    "\n",
    "# 3. Dimension analysis\n",
    "dimensions = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "sparsities = []\n",
    "similarities_avg = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    temp_encoder = HyperdimensionalEncoder(dimension=dim, sparsity=0.1, seed=42)\n",
    "    temp_hv1 = temp_encoder.probe_to_hypervector(sample_probes[0])\n",
    "    temp_hv2 = temp_encoder.probe_to_hypervector(sample_probes[1])\n",
    "    \n",
    "    sparsities.append(np.mean(temp_hv1 == 0))\n",
    "    similarities_avg.append(temp_encoder.compute_similarity(temp_hv1, temp_hv2))\n",
    "\n",
    "axes[1,0].plot(dimensions, sparsities, 'bo-', label='Actual Sparsity')\n",
    "axes[1,0].axhline(0.1, color='red', linestyle='--', label='Target Sparsity')\n",
    "axes[1,0].set_title('Sparsity vs Dimension')\n",
    "axes[1,0].set_xlabel('Dimension')\n",
    "axes[1,0].set_ylabel('Sparsity')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].set_xscale('log')\n",
    "\n",
    "# 4. Complexity encoding\n",
    "complexities = [probe['complexity'] for probe in sample_probes]\n",
    "norms = [np.linalg.norm(hv) for hv in probe_vectors]\n",
    "\n",
    "axes[1,1].scatter(complexities, norms, c=range(len(complexities)), cmap='plasma', s=100)\n",
    "axes[1,1].set_title('Complexity vs Vector Norm')\n",
    "axes[1,1].set_xlabel('Probe Complexity')\n",
    "axes[1,1].set_ylabel('Hypervector Norm')\n",
    "\n",
    "for i, (x, y) in enumerate(zip(complexities, norms)):\n",
    "    axes[1,1].annotate(domains[i], (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üå°Ô∏è Variance Tensor Visualization\n",
    "\n",
    "The variance tensor captures how model behavior changes across different layers and perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a variance tensor (in real use, this comes from REV executor)\n",
    "def create_simulated_variance_tensor(layers=20, challenges=50):\n",
    "    \"\"\"Create a simulated variance tensor for visualization.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Base variance pattern\n",
    "    base_variance = np.random.exponential(scale=0.1, size=(layers, challenges))\n",
    "    \n",
    "    # Add some structure - higher variance in middle layers\n",
    "    layer_weights = np.exp(-((np.arange(layers) - layers//2) / (layers//4))**2)\n",
    "    base_variance = base_variance * layer_weights.reshape(-1, 1)\n",
    "    \n",
    "    # Add challenge complexity effects\n",
    "    challenge_complexities = np.random.uniform(1, 5, challenges)\n",
    "    complexity_weights = challenge_complexities / 5.0\n",
    "    base_variance = base_variance * complexity_weights.reshape(1, -1)\n",
    "    \n",
    "    # Add some hotspots\n",
    "    hotspot_layers = [5, 12, 17]\n",
    "    hotspot_challenges = [10, 25, 35, 42]\n",
    "    \n",
    "    for layer in hotspot_layers:\n",
    "        for challenge in hotspot_challenges:\n",
    "            base_variance[layer, challenge] *= 3.0\n",
    "    \n",
    "    return base_variance, challenge_complexities\n",
    "\n",
    "# Create simulated data\n",
    "variance_tensor, complexities = create_simulated_variance_tensor()\n",
    "print(f\"üìä Created variance tensor: {variance_tensor.shape}\")\n",
    "print(f\"üìà Variance range: {variance_tensor.min():.3f} - {variance_tensor.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive variance visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Variance Tensor Heatmap',\n",
    "        'Layer-wise Variance Profile', \n",
    "        'Challenge Complexity vs Variance',\n",
    "        'Variance Distribution'\n",
    "    ],\n",
    "    specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "           [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    ")\n",
    "\n",
    "# 1. Main heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=variance_tensor,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Variance\", x=0.48)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Layer-wise profile\n",
    "layer_variances = np.mean(variance_tensor, axis=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(layer_variances))),\n",
    "        y=layer_variances,\n",
    "        mode='lines+markers',\n",
    "        name='Mean Variance',\n",
    "        line=dict(color='blue', width=3)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Complexity vs variance\n",
    "challenge_variances = np.mean(variance_tensor, axis=0)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=complexities,\n",
    "        y=challenge_variances,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=challenge_variances,\n",
    "            colorscale='Plasma',\n",
    "            showscale=False\n",
    "        ),\n",
    "        name='Challenges'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=variance_tensor.flatten(),\n",
    "        nbinsx=50,\n",
    "        name='Variance Distribution',\n",
    "        marker_color='green',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Challenge Index\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Layer Index\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Layer Index\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Mean Variance\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Challenge Complexity\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Mean Variance\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Variance Value\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"üå°Ô∏è Model Behavioral Variance Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive 3D variance visualization\n",
    "layers = np.arange(variance_tensor.shape[0])\n",
    "challenges = np.arange(variance_tensor.shape[1])\n",
    "L, C = np.meshgrid(layers, challenges, indexing='ij')\n",
    "\n",
    "fig_3d = go.Figure(data=[go.Surface(\n",
    "    z=variance_tensor,\n",
    "    x=C,\n",
    "    y=L,\n",
    "    colorscale='Viridis',\n",
    "    opacity=0.8\n",
    ")])\n",
    "\n",
    "fig_3d.update_layout(\n",
    "    title='üé¢ 3D Variance Landscape',\n",
    "    scene=dict(\n",
    "        xaxis_title='Challenge Index',\n",
    "        yaxis_title='Layer Index', \n",
    "        zaxis_title='Variance',\n",
    "        camera=dict(\n",
    "            eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "        )\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "print(\"üé¢ Rotate the 3D plot to explore variance patterns!\")\n",
    "print(\"üîç Look for peaks (hotspots) and valleys (stable regions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üï∏Ô∏è Causal Graph Discovery Visualization\n",
    "\n",
    "HBT can discover causal relationships in model behavior through variance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate causal graph discovery\n",
    "def create_simulated_causal_graph():\n",
    "    \"\"\"Create a simulated causal graph for demonstration.\"\"\"\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes (behavioral components)\n",
    "    components = [\n",
    "        ('Input_Embedding', {'layer': 0, 'type': 'input', 'importance': 0.9}),\n",
    "        ('Attention_1', {'layer': 1, 'type': 'attention', 'importance': 0.8}),\n",
    "        ('Attention_2', {'layer': 2, 'type': 'attention', 'importance': 0.7}),\n",
    "        ('Attention_3', {'layer': 3, 'type': 'attention', 'importance': 0.6}),\n",
    "        ('FFN_1', {'layer': 1, 'type': 'feedforward', 'importance': 0.5}),\n",
    "        ('FFN_2', {'layer': 2, 'type': 'feedforward', 'importance': 0.6}),\n",
    "        ('FFN_3', {'layer': 3, 'type': 'feedforward', 'importance': 0.4}),\n",
    "        ('Output_Layer', {'layer': 4, 'type': 'output', 'importance': 1.0})\n",
    "    ]\n",
    "    \n",
    "    G.add_nodes_from(components)\n",
    "    \n",
    "    # Add edges (causal relationships)\n",
    "    edges = [\n",
    "        ('Input_Embedding', 'Attention_1', {'weight': 0.9, 'type': 'direct'}),\n",
    "        ('Attention_1', 'FFN_1', {'weight': 0.7, 'type': 'within_layer'}),\n",
    "        ('Attention_1', 'Attention_2', {'weight': 0.8, 'type': 'cross_layer'}),\n",
    "        ('FFN_1', 'Attention_2', {'weight': 0.4, 'type': 'residual'}),\n",
    "        ('Attention_2', 'FFN_2', {'weight': 0.6, 'type': 'within_layer'}),\n",
    "        ('Attention_2', 'Attention_3', {'weight': 0.7, 'type': 'cross_layer'}),\n",
    "        ('FFN_2', 'Attention_3', {'weight': 0.3, 'type': 'residual'}),\n",
    "        ('Attention_3', 'FFN_3', {'weight': 0.5, 'type': 'within_layer'}),\n",
    "        ('FFN_3', 'Output_Layer', {'weight': 0.8, 'type': 'final'}),\n",
    "        ('Attention_3', 'Output_Layer', {'weight': 0.9, 'type': 'final'})\n",
    "    ]\n",
    "    \n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Create the causal graph\n",
    "causal_graph = create_simulated_causal_graph()\n",
    "\n",
    "print(f\"üï∏Ô∏è Created causal graph with {len(causal_graph.nodes)} nodes and {len(causal_graph.edges)} edges\")\n",
    "print(f\"üìä Node types: {set(nx.get_node_attributes(causal_graph, 'type').values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize causal graph\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Graph layout\n",
    "pos = nx.spring_layout(causal_graph, k=2, iterations=50)\n",
    "\n",
    "# Color nodes by type\n",
    "node_colors = {\n",
    "    'input': 'lightblue',\n",
    "    'attention': 'lightcoral', \n",
    "    'feedforward': 'lightgreen',\n",
    "    'output': 'gold'\n",
    "}\n",
    "\n",
    "colors = [node_colors[causal_graph.nodes[node]['type']] for node in causal_graph.nodes()]\n",
    "sizes = [causal_graph.nodes[node]['importance'] * 1000 for node in causal_graph.nodes()]\n",
    "\n",
    "# Plot 1: Network structure\n",
    "nx.draw(causal_graph, pos, ax=axes[0],\n",
    "        node_color=colors,\n",
    "        node_size=sizes,\n",
    "        with_labels=True,\n",
    "        font_size=8,\n",
    "        font_weight='bold',\n",
    "        arrows=True,\n",
    "        arrowsize=20,\n",
    "        edge_color='gray',\n",
    "        alpha=0.8)\n",
    "\n",
    "axes[0].set_title('üï∏Ô∏è Causal Graph Structure', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=node_type.title()) \n",
    "                  for node_type, color in node_colors.items()]\n",
    "axes[0].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Plot 2: Edge weights heatmap\n",
    "# Create adjacency matrix\n",
    "nodes = list(causal_graph.nodes())\n",
    "adj_matrix = np.zeros((len(nodes), len(nodes)))\n",
    "\n",
    "for i, source in enumerate(nodes):\n",
    "    for j, target in enumerate(nodes):\n",
    "        if causal_graph.has_edge(source, target):\n",
    "            adj_matrix[i, j] = causal_graph.edges[source, target]['weight']\n",
    "\n",
    "im = axes[1].imshow(adj_matrix, cmap='Reds', aspect='auto')\n",
    "axes[1].set_xticks(range(len(nodes)))\n",
    "axes[1].set_yticks(range(len(nodes)))\n",
    "axes[1].set_xticklabels([node.replace('_', '\\n') for node in nodes], rotation=45, ha='right')\n",
    "axes[1].set_yticklabels([node.replace('_', '\\n') for node in nodes])\n",
    "axes[1].set_title('üå°Ô∏è Causal Strength Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=axes[1], label='Causal Strength')\n",
    "\n",
    "# Add values to heatmap\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(len(nodes)):\n",
    "        if adj_matrix[i, j] > 0:\n",
    "            axes[1].text(j, i, f'{adj_matrix[i, j]:.2f}', \n",
    "                        ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive network visualization with Plotly\n",
    "def plot_interactive_causal_graph(G):\n",
    "    \"\"\"Create interactive causal graph visualization.\"\"\"\n",
    "    \n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Create edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_info = []\n",
    "    \n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        weight = G.edges[edge]['weight']\n",
    "        edge_info.append(f\"{edge[0]} ‚Üí {edge[1]}<br>Weight: {weight:.2f}\")\n",
    "    \n",
    "    edge_trace = go.Scatter(x=edge_x, y=edge_y,\n",
    "                           line=dict(width=2, color='#888'),\n",
    "                           hoverinfo='none',\n",
    "                           mode='lines')\n",
    "    \n",
    "    # Create node trace\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    node_color = []\n",
    "    node_size = []\n",
    "    \n",
    "    color_map = {\n",
    "        'input': 'lightblue',\n",
    "        'attention': 'lightcoral',\n",
    "        'feedforward': 'lightgreen', \n",
    "        'output': 'gold'\n",
    "    }\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        \n",
    "        node_type = G.nodes[node]['type']\n",
    "        importance = G.nodes[node]['importance']\n",
    "        layer = G.nodes[node]['layer']\n",
    "        \n",
    "        node_text.append(f\"{node}<br>Type: {node_type}<br>Layer: {layer}<br>Importance: {importance:.2f}\")\n",
    "        node_color.append(importance)\n",
    "        node_size.append(importance * 30 + 10)\n",
    "    \n",
    "    node_trace = go.Scatter(x=node_x, y=node_y,\n",
    "                           mode='markers+text',\n",
    "                           hoverinfo='text',\n",
    "                           text=[node.replace('_', '<br>') for node in G.nodes()],\n",
    "                           textposition=\"middle center\",\n",
    "                           textfont=dict(size=8),\n",
    "                           hovertext=node_text,\n",
    "                           marker=dict(size=node_size,\n",
    "                                     color=node_color,\n",
    "                                     colorscale='Viridis',\n",
    "                                     colorbar=dict(title=\"Importance\"),\n",
    "                                     line=dict(width=2, color='black')))\n",
    "    \n",
    "    # Create the figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                   layout=go.Layout(\n",
    "                        title='üï∏Ô∏è Interactive Causal Graph<br>Hover over nodes for details',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        annotations=[ dict(\n",
    "                            text=\"Node size = importance<br>Color = importance<br>Arrows show causal flow\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002 ) ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        width=800,\n",
    "                        height=600))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display interactive graph\n",
    "interactive_fig = plot_interactive_causal_graph(causal_graph)\n",
    "interactive_fig.show()\n",
    "\n",
    "print(\"üéØ Hover over nodes to see detailed information!\")\n",
    "print(\"üîç Node size and color represent importance in the causal structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üîç Model Comparison Visualization\n",
    "\n",
    "Let's visualize how different models compare behaviorally using hyperdimensional signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate behavioral signatures for different models\n",
    "def create_model_signatures(num_models=5, dimension=1024):\n",
    "    \"\"\"Create simulated behavioral signatures for different models.\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'GPT-2 Small': {'params': '124M', 'family': 'gpt', 'size': 'small'},\n",
    "        'GPT-2 Medium': {'params': '355M', 'family': 'gpt', 'size': 'medium'},\n",
    "        'GPT-2 Large': {'params': '774M', 'family': 'gpt', 'size': 'large'},\n",
    "        'BERT Base': {'params': '110M', 'family': 'bert', 'size': 'base'},\n",
    "        'T5 Small': {'params': '60M', 'family': 't5', 'size': 'small'}\n",
    "    }\n",
    "    \n",
    "    signatures = {}\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for model_name, info in models.items():\n",
    "        # Create base signature\n",
    "        signature = np.random.randn(dimension) * 0.1\n",
    "        \n",
    "        # Add family-specific patterns\n",
    "        if info['family'] == 'gpt':\n",
    "            # GPT models have certain activation patterns\n",
    "            signature[100:200] += np.random.exponential(0.5, 100)\n",
    "            signature[500:600] += np.random.normal(0.2, 0.1, 100)\n",
    "        elif info['family'] == 'bert':\n",
    "            # BERT has different patterns\n",
    "            signature[200:300] += np.random.exponential(0.3, 100)\n",
    "            signature[700:800] += np.random.normal(-0.1, 0.15, 100)\n",
    "        elif info['family'] == 't5':\n",
    "            # T5 has encoder-decoder patterns\n",
    "            signature[300:400] += np.random.exponential(0.4, 100)\n",
    "            signature[400:500] += np.random.normal(0.3, 0.1, 100)\n",
    "        \n",
    "        # Add size-specific scaling\n",
    "        if info['size'] == 'small':\n",
    "            signature *= 0.8\n",
    "        elif info['size'] == 'large':\n",
    "            signature *= 1.2\n",
    "        \n",
    "        # Ensure sparsity\n",
    "        mask = np.random.random(dimension) > 0.9\n",
    "        signature[mask] = 0\n",
    "        \n",
    "        signatures[model_name] = {\n",
    "            'signature': signature,\n",
    "            'info': info\n",
    "        }\n",
    "    \n",
    "    return signatures\n",
    "\n",
    "# Create model signatures\n",
    "model_signatures = create_model_signatures()\n",
    "print(f\"üìä Created signatures for {len(model_signatures)} models\")\n",
    "\n",
    "# Extract signatures and model names\n",
    "signatures_array = np.array([data['signature'] for data in model_signatures.values()])\n",
    "model_names = list(model_signatures.keys())\n",
    "\n",
    "print(f\"üìê Signatures shape: {signatures_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üîç Model Behavioral Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Similarity matrix\n",
    "similarity_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "for i, sig_i in enumerate(signatures_array):\n",
    "    for j, sig_j in enumerate(signatures_array):\n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(sig_i, sig_j) / (np.linalg.norm(sig_i) * np.linalg.norm(sig_j))\n",
    "        similarity_matrix[i, j] = similarity\n",
    "\n",
    "im1 = axes[0,0].imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0,0].set_title('Model Similarity Matrix')\n",
    "axes[0,0].set_xticks(range(len(model_names)))\n",
    "axes[0,0].set_yticks(range(len(model_names)))\n",
    "axes[0,0].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=45)\n",
    "axes[0,0].set_yticklabels([name.replace(' ', '\\n') for name in model_names])\n",
    "\n",
    "# Add similarity values\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(model_names)):\n",
    "        text = axes[0,0].text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\" if abs(similarity_matrix[i, j]) < 0.5 else \"white\")\n",
    "\n",
    "plt.colorbar(im1, ax=axes[0,0], label='Similarity')\n",
    "\n",
    "# 2. 2D projection using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=3)\n",
    "signatures_2d = tsne.fit_transform(signatures_array)\n",
    "\n",
    "# Color by family\n",
    "family_colors = {'gpt': 'red', 'bert': 'blue', 't5': 'green'}\n",
    "colors = [family_colors[model_signatures[name]['info']['family']] for name in model_names]\n",
    "\n",
    "scatter = axes[0,1].scatter(signatures_2d[:, 0], signatures_2d[:, 1], \n",
    "                           c=colors, s=100, alpha=0.7)\n",
    "axes[0,1].set_title('2D Behavioral Space (t-SNE)')\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(model_names):\n",
    "    axes[0,1].annotate(name.replace(' ', '\\n'), \n",
    "                      (signatures_2d[i, 0], signatures_2d[i, 1]),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Create legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=color, markersize=10, label=family)\n",
    "                  for family, color in family_colors.items()]\n",
    "axes[0,1].legend(handles=legend_elements, title='Model Family')\n",
    "\n",
    "# 3. Signature magnitude comparison\n",
    "signature_norms = [np.linalg.norm(sig) for sig in signatures_array]\n",
    "param_sizes = [model_signatures[name]['info']['params'] for name in model_names]\n",
    "\n",
    "bars = axes[1,0].bar(range(len(model_names)), signature_norms, \n",
    "                    color=[family_colors[model_signatures[name]['info']['family']] for name in model_names],\n",
    "                    alpha=0.7)\n",
    "axes[1,0].set_title('Behavioral Signature Magnitude')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('Signature Norm')\n",
    "axes[1,0].set_xticks(range(len(model_names)))\n",
    "axes[1,0].set_xticklabels([name.replace(' ', '\\n') for name in model_names], rotation=45)\n",
    "\n",
    "# Add parameter count labels on bars\n",
    "for i, (bar, params) in enumerate(zip(bars, param_sizes)):\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                  params, ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# 4. Activation pattern comparison\n",
    "# Show first 200 dimensions for visualization\n",
    "subset_sigs = signatures_array[:, :200]\n",
    "im2 = axes[1,1].imshow(subset_sigs, cmap='RdBu_r', aspect='auto')\n",
    "axes[1,1].set_title('Activation Patterns (First 200 Dims)')\n",
    "axes[1,1].set_xlabel('Dimension')\n",
    "axes[1,1].set_ylabel('Model')\n",
    "axes[1,1].set_yticks(range(len(model_names)))\n",
    "axes[1,1].set_yticklabels([name.replace(' ', '\\n') for name in model_names])\n",
    "\n",
    "plt.colorbar(im2, ax=axes[1,1], label='Activation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ‚ö° REV Executor Memory Pattern Visualization\n",
    "\n",
    "The REV executor uses memory-bounded sliding windows for scalable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate REV executor memory patterns\n",
    "def simulate_rev_execution(model_layers=24, window_size=6, stride=3, challenges=10):\n",
    "    \"\"\"Simulate REV executor memory-bounded execution.\"\"\"\n",
    "    \n",
    "    execution_log = []\n",
    "    memory_usage = []\n",
    "    timestamps = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate sliding window execution\n",
    "    for challenge in range(challenges):\n",
    "        for window_start in range(0, model_layers - window_size + 1, stride):\n",
    "            window_end = window_start + window_size\n",
    "            \n",
    "            # Simulate memory usage (varies by window complexity)\n",
    "            base_memory = 100  # MB\n",
    "            layer_complexity = np.sum(np.random.exponential(0.5, window_size))\n",
    "            memory_used = base_memory * (1 + layer_complexity / window_size)\n",
    "            \n",
    "            # Add some variance\n",
    "            memory_used += np.random.normal(0, memory_used * 0.1)\n",
    "            memory_used = max(memory_used, 50)  # Minimum memory\n",
    "            \n",
    "            # Simulate processing time\n",
    "            processing_time = np.random.exponential(2.0) + 0.5\n",
    "            \n",
    "            execution_log.append({\n",
    "                'challenge': challenge,\n",
    "                'window_start': window_start,\n",
    "                'window_end': window_end,\n",
    "                'layers': list(range(window_start, window_end)),\n",
    "                'memory_mb': memory_used,\n",
    "                'processing_time': processing_time,\n",
    "                'timestamp': len(execution_log) * 0.1\n",
    "            })\n",
    "            \n",
    "            memory_usage.append(memory_used)\n",
    "            timestamps.append(len(execution_log) * 0.1)\n",
    "    \n",
    "    return execution_log, memory_usage, timestamps\n",
    "\n",
    "# Run simulation\n",
    "execution_log, memory_usage, timestamps = simulate_rev_execution()\n",
    "print(f\"‚ö° Simulated {len(execution_log)} REV execution windows\")\n",
    "print(f\"üìä Memory usage range: {min(memory_usage):.1f} - {max(memory_usage):.1f} MB\")\n",
    "print(f\"‚è±Ô∏è Total simulation time: {max(timestamps):.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive REV visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Memory Usage Over Time',\n",
    "        'Sliding Window Coverage',\n",
    "        'Processing Time Distribution', \n",
    "        'Memory vs Window Position'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 1. Memory usage over time\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=timestamps,\n",
    "        y=memory_usage,\n",
    "        mode='lines+markers',\n",
    "        name='Memory Usage',\n",
    "        line=dict(color='blue', width=2),\n",
    "        marker=dict(size=4)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add memory limit line\n",
    "memory_limit = 800  # MB\n",
    "fig.add_hline(y=memory_limit, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=\"Memory Limit\", row=1, col=1)\n",
    "\n",
    "# 2. Sliding window coverage heatmap\n",
    "# Create coverage matrix\n",
    "max_layers = 24\n",
    "num_challenges = max(log['challenge'] for log in execution_log) + 1\n",
    "coverage_matrix = np.zeros((num_challenges, max_layers))\n",
    "\n",
    "for log_entry in execution_log:\n",
    "    challenge = log_entry['challenge']\n",
    "    for layer in log_entry['layers']:\n",
    "        coverage_matrix[challenge, layer] = log_entry['memory_mb']\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=coverage_matrix,\n",
    "        colorscale='Viridis',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Processing time distribution\n",
    "processing_times = [log['processing_time'] for log in execution_log]\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=processing_times,\n",
    "        nbinsx=20,\n",
    "        name='Processing Times',\n",
    "        marker_color='green',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Memory vs window position\n",
    "window_positions = [log['window_start'] for log in execution_log]\n",
    "window_memories = [log['memory_mb'] for log in execution_log]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=window_positions,\n",
    "        y=window_memories,\n",
    "        mode='markers',\n",
    "        name='Windows',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=window_memories,\n",
    "            colorscale='Plasma',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Memory (MB)\", x=1.02)\n",
    "        )\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Memory (MB)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Layer Index\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Challenge Index\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Processing Time (seconds)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Window Start Layer\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Memory Usage (MB)\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"‚ö° REV Executor Memory-Bounded Execution Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"üìä Key observations:\")\n",
    "print(f\"   ‚Ä¢ Average memory usage: {np.mean(memory_usage):.1f} MB\")\n",
    "print(f\"   ‚Ä¢ Peak memory usage: {max(memory_usage):.1f} MB\")\n",
    "print(f\"   ‚Ä¢ Memory efficiency: {(memory_limit - max(memory_usage))/memory_limit*100:.1f}% headroom\")\n",
    "print(f\"   ‚Ä¢ Average processing time: {np.mean(processing_times):.2f} seconds\")\n",
    "print(f\"   ‚Ä¢ Total windows processed: {len(execution_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìà Scaling Analysis Visualization\n",
    "\n",
    "Let's visualize how HBT validation scales with model size and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate scaling analysis\n",
    "def simulate_scaling_analysis():\n",
    "    \"\"\"Simulate how HBT performance scales with model size.\"\"\"\n",
    "    \n",
    "    # Model sizes in parameters\n",
    "    model_sizes = [125e6, 350e6, 760e6, 1.5e9, 6e9, 13e9, 30e9, 65e9, 175e9]\n",
    "    model_names = ['GPT2-S', 'GPT2-M', 'GPT2-L', 'GPT2-XL', 'GPT-J', 'GPT-NeoX', 'GPT-3', 'LLaMA-65B', 'GPT-3.5']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for size, name in zip(model_sizes, model_names):\n",
    "        # Simulate HBT construction metrics\n",
    "        \n",
    "        # Construction time (sub-linear scaling)\n",
    "        base_time = 60  # seconds for smallest model\n",
    "        construction_time = base_time * (size / model_sizes[0]) ** 0.5  # O(‚àön) scaling\n",
    "        construction_time += np.random.normal(0, construction_time * 0.1)  # Add noise\n",
    "        \n",
    "        # Memory usage (sub-linear)\n",
    "        base_memory = 500  # MB for smallest model \n",
    "        memory_usage = base_memory * (size / model_sizes[0]) ** 0.6\n",
    "        memory_usage += np.random.normal(0, memory_usage * 0.05)\n",
    "        \n",
    "        # API cost (linear with challenges, not model size)\n",
    "        base_cost = 1.50\n",
    "        api_cost = base_cost + np.random.normal(0, 0.2)\n",
    "        \n",
    "        # Accuracy (improves with model size, plateaus)\n",
    "        base_accuracy = 0.85\n",
    "        size_factor = np.log(size / model_sizes[0]) / np.log(model_sizes[-1] / model_sizes[0])\n",
    "        accuracy = base_accuracy + (0.98 - base_accuracy) * (1 - np.exp(-size_factor * 3))\n",
    "        accuracy += np.random.normal(0, 0.01)\n",
    "        \n",
    "        # Behavioral complexity (increases with model size)\n",
    "        complexity = size_factor * 0.8 + 0.2\n",
    "        complexity += np.random.normal(0, 0.05)\n",
    "        \n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'size': size,\n",
    "            'construction_time': max(construction_time, 30),  # Minimum time\n",
    "            'memory_usage': max(memory_usage, 200),  # Minimum memory\n",
    "            'api_cost': max(api_cost, 0.5),  # Minimum cost\n",
    "            'accuracy': min(accuracy, 0.99),  # Maximum accuracy\n",
    "            'complexity': min(complexity, 1.0)  # Maximum complexity\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate scaling data\n",
    "scaling_results = simulate_scaling_analysis()\n",
    "print(f\"üìà Generated scaling analysis for {len(scaling_results)} models\")\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "scaling_df = pd.DataFrame(scaling_results)\n",
    "print(scaling_df[['name', 'size', 'construction_time', 'accuracy']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive scaling visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Construction Time Scaling (Target: O(‚àön))',\n",
    "        'Memory Usage Scaling',\n",
    "        'Accuracy vs Model Size',\n",
    "        'Cost-Effectiveness Analysis'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 1. Construction time scaling\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=scaling_df['size'],\n",
    "        y=scaling_df['construction_time'],\n",
    "        mode='markers+lines',\n",
    "        name='Actual',\n",
    "        line=dict(color='blue', width=2),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add theoretical O(‚àön) line\n",
    "theoretical_times = [60 * (size / scaling_df['size'].iloc[0]) ** 0.5 for size in scaling_df['size']]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=scaling_df['size'],\n",
    "        y=theoretical_times,\n",
    "        mode='lines',\n",
    "        name='Theoretical O(‚àön)',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Memory usage scaling\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=scaling_df['size'],\n",
    "        y=scaling_df['memory_usage'],\n",
    "        mode='markers+lines',\n",
    "        name='Memory Usage',\n",
    "        line=dict(color='green', width=2),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Accuracy vs model size\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=scaling_df['size'],\n",
    "        y=scaling_df['accuracy'],\n",
    "        mode='markers+lines',\n",
    "        name='Accuracy',\n",
    "        line=dict(color='orange', width=2),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add target accuracy line\n",
    "fig.add_hline(y=0.958, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=\"Paper Target (95.8%)\", row=2, col=1)\n",
    "\n",
    "# 4. Cost-effectiveness (accuracy per dollar)\n",
    "cost_effectiveness = scaling_df['accuracy'] / scaling_df['api_cost']\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=scaling_df['size'],\n",
    "        y=cost_effectiveness,\n",
    "        mode='markers+lines',\n",
    "        name='Accuracy per $',\n",
    "        line=dict(color='purple', width=2),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update axes to log scale for model size\n",
    "fig.update_xaxes(type=\"log\", title_text=\"Model Parameters\", row=1, col=1)\n",
    "fig.update_xaxes(type=\"log\", title_text=\"Model Parameters\", row=1, col=2)\n",
    "fig.update_xaxes(type=\"log\", title_text=\"Model Parameters\", row=2, col=1)\n",
    "fig.update_xaxes(type=\"log\", title_text=\"Model Parameters\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Construction Time (seconds)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Memory Usage (MB)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy per $\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"üìà HBT Scaling Analysis: Sub-Linear Performance\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print scaling analysis\n",
    "print(\"üìä Scaling Analysis Results:\")\n",
    "print(f\"   ‚Ä¢ Largest model: {scaling_df.iloc[-1]['name']} ({scaling_df.iloc[-1]['size']/1e9:.0f}B params)\")\n",
    "print(f\"   ‚Ä¢ Construction time ratio (largest/smallest): {scaling_df.iloc[-1]['construction_time']/scaling_df.iloc[0]['construction_time']:.1f}x\")\n",
    "print(f\"   ‚Ä¢ Theoretical O(n) ratio: {scaling_df.iloc[-1]['size']/scaling_df.iloc[0]['size']:.0f}x\")\n",
    "print(f\"   ‚Ä¢ Theoretical O(‚àön) ratio: {(scaling_df.iloc[-1]['size']/scaling_df.iloc[0]['size'])**0.5:.1f}x\")\n",
    "print(f\"   ‚Ä¢ Achieved scaling: Sub-linear ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Best accuracy: {scaling_df['accuracy'].max():.3f}\")\n",
    "print(f\"   ‚Ä¢ Most cost-effective: {scaling_df.loc[cost_effectiveness.idxmax(), 'name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "This notebook demonstrated the core concepts behind HBT validation:\n",
    "\n",
    "### üß† Hyperdimensional Computing\n",
    "- High-dimensional vectors (1K-100K dimensions) capture behavioral nuances\n",
    "- Sparsity (~10% non-zero) enables efficient computation\n",
    "- Similarity metrics reveal behavioral relationships between models\n",
    "\n",
    "### üå°Ô∏è Variance Analysis\n",
    "- Variance tensors reveal layer-wise behavioral patterns\n",
    "- Hotspots indicate critical decision points in models\n",
    "- Perturbation analysis discovers causal relationships\n",
    "\n",
    "### üï∏Ô∏è Causal Discovery\n",
    "- Graph structures emerge from behavioral correlations\n",
    "- Attention and feedforward patterns create distinct signatures\n",
    "- Cross-layer dependencies reveal model architecture\n",
    "\n",
    "### üîç Model Comparison\n",
    "- Behavioral signatures enable model family identification\n",
    "- 2D projections reveal clustering by architecture\n",
    "- Similarity matrices quantify behavioral equivalence\n",
    "\n",
    "### ‚ö° Scalable Execution\n",
    "- REV executor maintains sub-linear memory complexity\n",
    "- Sliding windows enable analysis of arbitrarily large models\n",
    "- Memory-bounded execution stays within practical limits\n",
    "\n",
    "### üìà Practical Scaling\n",
    "- O(‚àön) time complexity for model size n\n",
    "- Constant API costs regardless of model size\n",
    "- Accuracy improves with model size but plateaus\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready to try HBT validation on real models? Check out our [examples](../examples/) and [API documentation](../docs/)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ HBT Visualization Demo Complete!\")\n",
    "print(\"üìö Next steps:\")\n",
    "print(\"   ‚Ä¢ Try basic_verification.py with real models\")\n",
    "print(\"   ‚Ä¢ Run api_audit.py on commercial APIs\")\n",
    "print(\"   ‚Ä¢ Explore the research examples\")\n",
    "print(\"   ‚Ä¢ Read the full documentation\")\n",
    "print(\"\\nüî¨ Happy model validation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}